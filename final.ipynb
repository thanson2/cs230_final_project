{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz1MJrck3tZ1"
      },
      "outputs": [],
      "source": [
        "!pip install ta\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D, Conv1D\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "import yfinance as yf\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "import ta\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsnH-7pa4Duv"
      },
      "outputs": [],
      "source": [
        "def get_s_and_p_tickers():\n",
        "  s_and_p = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
        "  tickers = [symbol for symbol in s_and_p.Symbol.to_list() if str.isalpha(symbol)]\n",
        "  return tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_-_QR4GRvvt"
      },
      "outputs": [],
      "source": [
        "tickers = get_s_and_p_tickers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xHLsX-UT4X50"
      },
      "outputs": [],
      "source": [
        "def get_s_and_p(tickers):\n",
        "\n",
        "  ticker_data_frames = {}\n",
        "  remove = []\n",
        "  for ticker in tickers:\n",
        "\n",
        "      data = yf.download(ticker, period=\"5y\", interval=\"1d\")\n",
        "\n",
        "      if data.empty:\n",
        "        remove.append(ticker)\n",
        "        import time\n",
        "        time.sleep(15)\n",
        "        continue\n",
        "\n",
        "      close = data['Close'].squeeze()\n",
        "      volume = data['Volume'].squeeze()\n",
        "      high = data['High'].squeeze()\n",
        "      low = data['Low'].squeeze()\n",
        "      diff = data['Close'].diff(1).squeeze()\n",
        "      percent_change_close = (data['Close'].pct_change() * 100).squeeze()\n",
        "      volatility = (data['Close'].pct_change().std() * (252**0.5)).squeeze()\n",
        "      rolling_average_close = data['Close'].rolling(window=20).mean().squeeze()\n",
        "      time_index = np.arange(close.shape[0], 0, -1)\n",
        "\n",
        "      ticker_df = pd.DataFrame({\n",
        "          'close': close,\n",
        "          'high': high,\n",
        "          'low': low,\n",
        "          'time_index': time_index,\n",
        "          'volatility': volatility,\n",
        "          'rolling_average_close': rolling_average_close,\n",
        "          'percent_change_close': percent_change_close,\n",
        "          'volume': volume,\n",
        "          'diff': diff,\n",
        "      })\n",
        "\n",
        "      ticker_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "      ticker_df.dropna(inplace=True)\n",
        "\n",
        "      ticker_data_frames[ticker] = ticker_df\n",
        "\n",
        "  for ticker in remove:\n",
        "    tickers.remove(ticker)\n",
        "\n",
        "  return ticker_data_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UzlsVFzRSkj"
      },
      "outputs": [],
      "source": [
        "def get_s_and_p_additonal(tickers):\n",
        "    ticker_data_frames = {}\n",
        "    remove = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        try:\n",
        "            data = yf.download(ticker, period=\"5y\", interval=\"1d\")\n",
        "            financial_data = yf.Ticker(ticker)\n",
        "\n",
        "            if data.empty:\n",
        "                print(f\"No data found for {ticker}\")\n",
        "                remove.append(ticker)\n",
        "                continue\n",
        "\n",
        "            close = data['Close'].squeeze()\n",
        "            volume = data['Volume'].squeeze()\n",
        "            high = data['High'].squeeze()\n",
        "            low = data['Low'].squeeze()\n",
        "            diff = data['Close'].diff(1).squeeze()\n",
        "            percent_change_close = (data['Close'].pct_change() * 100).squeeze()\n",
        "            volatility = (data['Close'].pct_change().std() * (252**0.5)).squeeze()\n",
        "            rolling_average_close = data['Close'].rolling(window=20).mean().squeeze()\n",
        "            time_index = np.arange(close.shape[0], 0, -1)\n",
        "\n",
        "            ticker_df = pd.DataFrame({\n",
        "                'close': close,\n",
        "                'high': high,\n",
        "                'low': low,\n",
        "                'time_index': time_index,\n",
        "                'volatility': volatility,\n",
        "                'rolling_average_close': rolling_average_close,\n",
        "                'percent_change_close': percent_change_close,\n",
        "                'volume': volume,\n",
        "                'diff': diff,\n",
        "            })\n",
        "\n",
        "            # Volume Indicators\n",
        "            ticker_df['volume_adi'] = ta.volume.acc_dist_index(high, low, close, volume)\n",
        "            ticker_df['volume_obv'] = ta.volume.on_balance_volume(close, volume)\n",
        "            ticker_df['volume_cmf'] = ta.volume.chaikin_money_flow(high, low, close, volume)\n",
        "            ticker_df['volume_fi'] = ta.volume.force_index(close, volume)\n",
        "            ticker_df['volume_em'] = ta.volume.ease_of_movement(high, low, volume)\n",
        "            ticker_df['volume_sma_em'] = ta.volume.sma_ease_of_movement(high, low, volume)\n",
        "            ticker_df['volume_vpt'] = ta.volume.volume_price_trend(close, volume)\n",
        "            ticker_df['volume_nvi'] = ta.volume.negative_volume_index(close, volume)\n",
        "            ticker_df['volume_vwap'] = ta.volume.volume_weighted_average_price(high, low, close, volume)\n",
        "            ticker_df['volume_mfi'] = ta.volume.money_flow_index(high, low, close, volume)\n",
        "\n",
        "            # Volatility Indicators\n",
        "            ticker_df['volatility_atr'] = ta.volatility.average_true_range(high, low, close)\n",
        "            ticker_df['volatility_bbm'] = ta.volatility.bollinger_mavg(close)\n",
        "            ticker_df['volatility_bbh'] = ta.volatility.bollinger_hband(close)\n",
        "            ticker_df['volatility_bbl'] = ta.volatility.bollinger_lband(close)\n",
        "            ticker_df['volatility_bbhi'] = ta.volatility.bollinger_hband_indicator(close)\n",
        "            ticker_df['volatility_bbli'] = ta.volatility.bollinger_lband_indicator(close)\n",
        "            ticker_df['volatility_bbw'] = ta.volatility.bollinger_wband(close)\n",
        "            ticker_df['volatility_bbp'] = ta.volatility.bollinger_pband(close)\n",
        "            ticker_df['volatility_kcp'] = ta.volatility.keltner_channel_mband(high, low, close)\n",
        "            ticker_df['volatility_kch'] = ta.volatility.keltner_channel_hband(high, low, close)\n",
        "            ticker_df['volatility_kcl'] = ta.volatility.keltner_channel_lband(high, low, close)\n",
        "            ticker_df['volatility_kchi'] = ta.volatility.keltner_channel_hband_indicator(high, low, close)\n",
        "            ticker_df['volatility_kcli'] = ta.volatility.keltner_channel_lband_indicator(high, low, close)\n",
        "            ticker_df['volatility_kcw'] = ta.volatility.keltner_channel_wband(high, low, close)\n",
        "            ticker_df['volatility_kcp'] = ta.volatility.keltner_channel_pband(high, low, close)\n",
        "            ticker_df['volatility_dcl'] = ta.volatility.donchian_channel_lband(high, low, close)\n",
        "            ticker_df['volatility_dch'] = ta.volatility.donchian_channel_hband(high, low, close)\n",
        "            ticker_df['volatility_dcm'] = ta.volatility.donchian_channel_mband(high, low, close)\n",
        "            ticker_df['volatility_dcw'] = ta.volatility.donchian_channel_wband(high, low, close)\n",
        "            ticker_df['volatility_dcp'] = ta.volatility.donchian_channel_pband(high, low, close)\n",
        "            ticker_df['volatility_ui'] = ta.volatility.ulcer_index(close)\n",
        "\n",
        "            # Trend Indicators\n",
        "            ticker_df['trend_macd'] = ta.trend.macd(close)\n",
        "            ticker_df['trend_macd_signal'] = ta.trend.macd_signal(close)\n",
        "            ticker_df['trend_macd_diff'] = ta.trend.macd_diff(close)\n",
        "            ticker_df['trend_adx'] = ta.trend.adx(high, low, close)\n",
        "            ticker_df['trend_adx_pos'] = ta.trend.adx_pos(high, low, close)\n",
        "            ticker_df['trend_adx_neg'] = ta.trend.adx_neg(high, low, close)\n",
        "            ticker_df['trend_vortex_ind_pos'] = ta.trend.vortex_indicator_pos(high, low, close)\n",
        "            ticker_df['trend_vortex_ind_neg'] = ta.trend.vortex_indicator_neg(high, low, close)\n",
        "            ticker_df['trend_trix'] = ta.trend.trix(close)\n",
        "            ticker_df['trend_mass_index'] = ta.trend.mass_index(high, low)\n",
        "            ticker_df['trend_cci'] = ta.trend.cci(high, low, close)\n",
        "            ticker_df['trend_dpo'] = ta.trend.dpo(close)\n",
        "            ticker_df['trend_kst'] = ta.trend.kst(close)\n",
        "            ticker_df['trend_kst_sig'] = ta.trend.kst_sig(close)\n",
        "            ticker_df['trend_ichimoku_a'] = ta.trend.ichimoku_a(high, low)\n",
        "            ticker_df['trend_ichimoku_b'] = ta.trend.ichimoku_b(high, low)\n",
        "            ticker_df['trend_ichimoku_base'] = ta.trend.ichimoku_base_line(high, low)\n",
        "            ticker_df['trend_ichimoku_conv'] = ta.trend.ichimoku_conversion_line(high, low)\n",
        "            ticker_df['trend_sma'] = ta.trend.sma_indicator(close)\n",
        "            ticker_df['trend_ema'] = ta.trend.ema_indicator(close)\n",
        "            ticker_df['trend_wma'] = ta.trend.wma_indicator(close)\n",
        "            ticker_df['trend_aroon_up'] = ta.trend.aroon_up(high, low)\n",
        "            ticker_df['trend_aroon_down'] = ta.trend.aroon_down(high, low)\n",
        "\n",
        "            # Momentum Indicators\n",
        "            ticker_df['momentum_rsi'] = ta.momentum.rsi(close)\n",
        "            ticker_df['momentum_stoch_rsi'] = ta.momentum.stochrsi(close)\n",
        "            ticker_df['momentum_stoch_rsi_k'] = ta.momentum.stochrsi_k(close)\n",
        "            ticker_df['momentum_stoch_rsi_d'] = ta.momentum.stochrsi_d(close)\n",
        "            ticker_df['momentum_stoch'] = ta.momentum.stoch(high, low, close)\n",
        "            ticker_df['momentum_stoch_signal'] = ta.momentum.stoch_signal(high, low, close)\n",
        "            ticker_df['momentum_tsi'] = ta.momentum.tsi(close)\n",
        "            ticker_df['momentum_ultimate_oscillator'] = ta.momentum.ultimate_oscillator(high, low, close)\n",
        "            ticker_df['momentum_ao'] = ta.momentum.awesome_oscillator(high, low)\n",
        "            ticker_df['momentum_kama'] = ta.momentum.kama(close)\n",
        "            ticker_df['momentum_roc'] = ta.momentum.roc(close)\n",
        "            ticker_df['momentum_ppo'] = ta.momentum.ppo(close)\n",
        "            ticker_df['momentum_ppo_signal'] = ta.momentum.ppo_signal(close)\n",
        "            ticker_df['momentum_ppo_hist'] = ta.momentum.ppo_hist(close)\n",
        "            ticker_df['momentum_pvo'] = ta.momentum.pvo(volume)\n",
        "            ticker_df['momentum_pvo_signal'] = ta.momentum.pvo_signal(volume)\n",
        "            ticker_df['momentum_pvo_hist'] = ta.momentum.pvo_hist(volume)\n",
        "            ticker_df['momentum_wr'] = ta.momentum.williams_r(high, low, close)\n",
        "\n",
        "            # Get fundamental data\n",
        "\n",
        "            financial_data = yf.Ticker(ticker).info\n",
        "\n",
        "            # Add fundamental metrics to DataFrame\n",
        "            metrics = {\n",
        "                'forward_pe': financial_data.get('forwardPE', np.nan),\n",
        "                'target_median_price': financial_data.get('targetMedianPrice', np.nan),\n",
        "                'target_mean_price': financial_data.get('targetMeanPrice', np.nan),\n",
        "                'current_ratio': financial_data.get('currentRatio', np.nan),\n",
        "                #'trailing_peg_ratio': financial_data.get('trailingPegRatio', np.nan),\n",
        "                'short_date': financial_data.get('dateShortInterest', np.nan),\n",
        "                'price_to_book': financial_data.get('priceToBook', np.nan),\n",
        "                'enterprise_to_revenue': financial_data.get('enterpriseToRevenue', np.nan),\n",
        "                'enterprise_to_ebitda': financial_data.get('enterpriseToEbitda', np.nan),\n",
        "                'year_change': financial_data.get('52WeekChange', np.nan),\n",
        "                'roa': financial_data.get('returnOnAssets', np.nan),\n",
        "                'roe': financial_data.get('returnOnEquity', np.nan),\n",
        "                'revenue_growth': financial_data.get('revenueGrowth', np.nan),\n",
        "                'gross_margins': financial_data.get('grossMargins', np.nan),\n",
        "                'ebitda_margins': financial_data.get('ebitdaMargins', np.nan),\n",
        "                'operating_margins': financial_data.get('operatingMargins', np.nan),\n",
        "                #'trailing_pe': financial_data.get('trailingPE', np.nan),\n",
        "                'beta': financial_data.get('beta', np.nan),\n",
        "                'audit_risk': financial_data.get('auditRisk', np.nan),\n",
        "                'board_risk': financial_data.get('boardRisk', np.nan),\n",
        "                'overall_risk': financial_data.get('overallRisk', np.nan),\n",
        "                'average_volume': financial_data.get('averageVolume', np.nan),\n",
        "                'average_volume_10d': financial_data.get('averageVolume10days', np.nan),\n",
        "                'average_daily_volume_10d': financial_data.get('averageDailyVolume10Day', np.nan),\n",
        "                'price_to_sales': financial_data.get('priceToSalesTrailing12Months', np.nan),\n",
        "                'short_ratio': financial_data.get('shortRatio', np.nan),\n",
        "                'short_percent_float': financial_data.get('shortPercentOfFloat', np.nan)\n",
        "            }\n",
        "\n",
        "            # Fill DataFrame with constant values for all rows\n",
        "            for metric, value in metrics.items():\n",
        "                ticker_df[metric] = value\n",
        "\n",
        "            # Clean up the data\n",
        "            ticker_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            ticker_df = ticker_df.infer_objects()\n",
        "            ticker_df = ticker_df.apply(lambda col: col.fillna(0) if col.dtype != 'object' else col.fillna(''))\n",
        "\n",
        "            # Add to dictionary\n",
        "            ticker_data_frames[ticker] = ticker_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {ticker}: {str(e)}\")\n",
        "            remove.append(ticker)\n",
        "            continue\n",
        "\n",
        "\n",
        "    for ticker in remove:\n",
        "        tickers.remove(ticker)\n",
        "\n",
        "    return ticker_data_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZ3jnvHRXek"
      },
      "outputs": [],
      "source": [
        "ticker_data_frames = get_s_and_p(tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ27IVeZguJJ"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "def zeroMaxScaler(data, train_percent):\n",
        "  epsilon = 0.001\n",
        "  max = np.maximum(np.max(data[0: math.floor(data.shape[0] * train_percent)], axis=(0, 1)).reshape(1, 1, -1), epsilon)\n",
        "  return data / max\n",
        "\n",
        "\n",
        "def create_sequences(\n",
        "    dataframe_dict: dict,\n",
        "    tickers: list,\n",
        "    train_percent: float = .8,\n",
        "    sequence_length: int = 20,\n",
        "  ):\n",
        "\n",
        "    epsilon = 1e-12\n",
        "\n",
        "    #scaler = MinMaxScaler()\n",
        "    scaler = StandardScaler()\n",
        "    #scaler = MaxAbsScaler()\n",
        "\n",
        "    sequence_dict = {}\n",
        "    sequence_label_dict = {}\n",
        "    for ticker in tickers:\n",
        "      dataframe = dataframe_dict[ticker]\n",
        "      numerical_columns = dataframe.select_dtypes(include=np.number).columns\n",
        "      data = dataframe[numerical_columns].to_numpy()\n",
        "      N, D = data.shape\n",
        "      L = sequence_length\n",
        "\n",
        "      sequences = sliding_window_view(data, window_shape=L, axis=0)[:-1]\n",
        "\n",
        "      mean = sequences.mean(axis=2, keepdims=True)\n",
        "      std = sequences.std(axis=2, keepdims=True)\n",
        "\n",
        "      normalized_sequences = (sequences - mean) / (std + epsilon)\n",
        "      sequence_dict[ticker] = normalized_sequences\n",
        "\n",
        "      close_index = dataframe.columns.get_loc('close')\n",
        "\n",
        "      last_day_indices = np.arange(L - 1, N - 1)\n",
        "      next_day_indices = last_day_indices + 1\n",
        "      last_day_close_values = data[last_day_indices, close_index]\n",
        "\n",
        "      next_day_close_values = data[next_day_indices, close_index]\n",
        "\n",
        "      mean_close = mean[:, close_index, 0]\n",
        "      std_close = std[:, close_index, 0]\n",
        "\n",
        "      last_day_close = (last_day_close_values - mean_close) / (std_close + epsilon)\n",
        "      next_day_close = (next_day_close_values - mean_close) / (std_close + epsilon)\n",
        "\n",
        "      sequence_labels = np.column_stack((last_day_close, next_day_close, mean_close, std_close))\n",
        "      sequence_label_dict[ticker] = sequence_labels\n",
        "\n",
        "    return sequence_dict, sequence_label_dict\n",
        "\n",
        "\n",
        "def train_test_split(\n",
        "    ticker_data_frames: dict,\n",
        "    tickers: list,\n",
        "    train_percent: float = .8,\n",
        "    sequence_length: int = 20,\n",
        "):\n",
        "\n",
        "  train_data = []\n",
        "  train_labels = []\n",
        "  dev_data = []\n",
        "  dev_labels = []\n",
        "  test_data = []\n",
        "  test_labels = []\n",
        "\n",
        "  sequence_dict, label_dict = create_sequences(ticker_data_frames, tickers, train_percent, sequence_length)\n",
        "\n",
        "  for ticker in sequence_dict.keys():\n",
        "    sequence_len = len(sequence_dict[ticker])\n",
        "    train_data.append(sequence_dict[ticker][:math.floor(sequence_len * train_percent)])\n",
        "    hold_out_data = sequence_dict[ticker][math.floor(sequence_len * train_percent):]\n",
        "    dev_data.append(hold_out_data[len(hold_out_data) // 2:])\n",
        "    test_data.append(hold_out_data[:len(hold_out_data) // 2])\n",
        "\n",
        "    train_labels.append(label_dict[ticker][:math.floor(sequence_len * train_percent)])\n",
        "    hold_out_labels = label_dict[ticker][math.floor(sequence_len * train_percent):]\n",
        "    dev_labels.append(hold_out_labels[len(hold_out_data) // 2:])\n",
        "    test_labels.append(hold_out_labels[:len(hold_out_labels) // 2])\n",
        "\n",
        "  train_data = np.concatenate(train_data, axis=0)\n",
        "  train_labels = np.concatenate(train_labels, axis=0)\n",
        "  dev_data = np.concatenate(dev_data, axis=0)\n",
        "  dev_labels = np.concatenate(dev_labels, axis=0)\n",
        "  test_data = np.concatenate(test_data, axis=0)\n",
        "  test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "  return train_data, train_labels, dev_data, dev_labels, test_data, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g8iB_jGsRtiX"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels, dev_data, dev_labels, test_data, test_labels = train_test_split(ticker_data_frames, tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmhAsTfILG50"
      },
      "outputs": [],
      "source": [
        "def mae_loss(y_true, y_pred):\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    abs_error = tf.abs(y_true_next - y_pred_next)\n",
        "\n",
        "    return tf.reduce_mean(abs_error)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    mse_error = (y_true_next - y_pred_next) ** 2\n",
        "\n",
        "    return tf.reduce_mean(mse_error)\n",
        "\n",
        "def dir_acc(y_true, y_pred):\n",
        "    y_true_prev = tf.cast(y_true[:, 0], tf.float64)\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "\n",
        "    true_change = y_true_next - y_true_prev\n",
        "    pred_change = y_pred_next - y_true_prev\n",
        "\n",
        "    correct_direction = tf.equal(tf.sign(true_change), tf.sign(pred_change))\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(correct_direction, tf.float64))\n",
        "\n",
        "\n",
        "def percent_error_true(y_true, y_pred):\n",
        "    epsilon = 1e-6\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    percent_error = abs(y_true_next - y_pred_next) / (y_true_next + epsilon)\n",
        "    return tf.reduce_mean(tf.cast(percent_error, tf.float64))\n",
        "\n",
        "def percent_error_adjusted(y_true, y_pred): #Use this one for 0 - 1 scaler\n",
        "    threshold = 1e-3\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    percent_error = tf.where(\n",
        "        y_true_next > threshold,\n",
        "        abs(y_true_next - y_pred_next) / (y_true_next),\n",
        "        tf.zeros_like(y_true_next)\n",
        "    )\n",
        "    return tf.reduce_mean(tf.cast(percent_error, tf.float64))\n",
        "\n",
        "def percent_error_squared_adjusted(y_true, y_pred):\n",
        "    threshold = 1e-3\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    percent_error = tf.where(\n",
        "        y_true_next > threshold,\n",
        "        abs(y_true_next - y_pred_next) / (y_true_next),\n",
        "        tf.zeros_like(y_true_next)\n",
        "    )\n",
        "    return tf.reduce_mean(tf.cast(percent_error, tf.float64))\n",
        "\n",
        "def percent_error_unnormalized(y_true, y_pred): #Use this one for std scaler\n",
        "    threshold = 1e-3\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64) * tf.cast(y_true[:, 3], tf.float64) + tf.cast(y_true[:, 2], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64) * tf.cast(y_true[:, 3], tf.float64) + tf.cast(y_true[:, 2], tf.float64)\n",
        "    percent_error = tf.where(\n",
        "        tf.math.abs(y_true_next) > threshold,\n",
        "        abs(y_true_next - y_pred_next) / y_true_next,\n",
        "        tf.zeros_like(y_true_next),\n",
        "    )\n",
        "    return tf.reduce_mean(percent_error)\n",
        "\n",
        "def percent_error_squared_unnormalized(y_true, y_pred):\n",
        "    threshold = 1e-3\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64) * tf.cast(y_true[:, 3], tf.float64) + tf.cast(y_true[:, 2], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64) * tf.cast(y_true[:, 3], tf.float64) + tf.cast(y_true[:, 2], tf.float64)\n",
        "    percent_error = tf.where(\n",
        "        tf.math.abs(y_true_next) > threshold,\n",
        "        (y_true_next - y_pred_next) / y_true_next,\n",
        "        tf.zeros_like(y_true_next),\n",
        "    )\n",
        "    return tf.reduce_mean(tf.square(percent_error))\n",
        "\n",
        "\n",
        "def error_direction(y_true, y_pred):\n",
        "    y_true_next = tf.cast(y_true[:, 1], tf.float64)\n",
        "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)\n",
        "    error_direction = tf.sign(y_true_next - y_pred_next)\n",
        "    return tf.reduce_mean(tf.cast(error_direction, tf.float64))\n",
        "\n",
        "meanSquaredLogarithmicError = MeanSquaredLogarithmicError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oVFgf5WK06z"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0, num_filters = 64, kernel_size = 3):\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = Conv1D(\n",
        "        filters=num_filters // 8,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = Conv1D(\n",
        "        filters=num_filters // 4,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = Conv1D(\n",
        "        filters=num_filters // 2,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = Conv1D(\n",
        "        filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = Conv1D(\n",
        "        filters=20,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = Add()([x, inputs])\n",
        "\n",
        "    y = LayerNormalization(epsilon=1e-6)(x)\n",
        "    y = Dense(ff_dim, activation=\"relu\",  kernel_regularizer=regularizers.L2(l2=1e-4))(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Dense(inputs.shape[-1], kernel_regularizer=regularizers.L2(l2=1e-4))(y)\n",
        "    return Add()([y, x])\n",
        "\n",
        "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    outputs = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def compile_model(\n",
        "  input_shape,\n",
        "  head_size,\n",
        "  num_heads,\n",
        "  ff_dim,\n",
        "  num_layers,\n",
        "  dropout,\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=percent_error_squared_adjusted,\n",
        "  metrics=[dir_acc, percent_error_true, error_direction, mse_loss, mae_loss],\n",
        "):\n",
        "  if os.path.exists(\"transformer_train_model.keras\"):\n",
        "    os.remove(\"transformer_train_model.keras\")\n",
        "  if os.path.exists(\"transformer_val_model.keras\"):\n",
        "    os.remove(\"transformer_val_model.keras\")\n",
        "\n",
        "  model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "  #model.summary()\n",
        "  return model\n",
        "\n",
        "def train_model(\n",
        "  model,\n",
        "  train_sequences,\n",
        "  train_labels,\n",
        "  validation_sequences,\n",
        "  validation_labels,\n",
        "  epochs: int,\n",
        "  batch_size: int,\n",
        "):\n",
        "\n",
        "  checkpoint_callback_val = ModelCheckpoint(\n",
        "      \"transformer_val_model.keras\",\n",
        "      monitor=\"val_loss\",\n",
        "      save_best_only=True,\n",
        "      mode=\"min\",\n",
        "      verbose=1,\n",
        "  )\n",
        "\n",
        "  model.fit(\n",
        "    train_sequences,\n",
        "    train_labels,\n",
        "    validation_data=(validation_sequences, validation_labels),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    callbacks=[checkpoint_callback_val],\n",
        "  )\n",
        "\n",
        "def evaluate_model(\n",
        "  model,\n",
        "  test_sequences,\n",
        "  test_labels,\n",
        "):\n",
        "\n",
        "  if os.path.exists(\"transformer_val_model.keras\"):\n",
        "    model.load_weights(\"transformer_val_model.keras\")\n",
        "  else:\n",
        "    print(\"No model found\")\n",
        "    return\n",
        "\n",
        "  accuracy = model.evaluate(test_sequences, test_labels)[1]\n",
        "  predictions = model.predict(test_sequences)\n",
        "  r2 = r2_score(test_labels[:, 1], predictions[:, 0])\n",
        "\n",
        "  return predictions, accuracy, r2\n",
        "\n",
        "# TODO future implementation\n",
        "# def iterative_evaluation(\n",
        "#   model,\n",
        "#   test_sequences,\n",
        "#   test_labels,\n",
        "# ):\n",
        "# # Lists to store predictions and actual values\n",
        "# predictions = []\n",
        "# actuals = []\n",
        "\n",
        "# # Loop over each time step starting from n_steps\n",
        "# for i in range(len(X)):\n",
        "#     X_input = X[i].reshape((1, n_steps, 1))\n",
        "\n",
        "#     # Predict on the current batch\n",
        "#     y_pred = model.predict(X_input, verbose=0)\n",
        "#     predictions.append(y_pred[0][0])\n",
        "#     actuals.append(y[i][0])\n",
        "\n",
        "#     # Train on the current batch\n",
        "#     model.fit(X_input, y[i].reshape(1, 1), epochs=1, batch_size=batch_size, verbose=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MSE\n",
        "optimizer=tf.keras.optimizers.Adam()\n",
        "model = compile_model(train_data.shape[1:], 16, 8, 128, 6, 0.20, optimizer=optimizer, loss=meanSquaredLogarithmicError)\n",
        "train_model(model, train_data, train_labels, dev_data, dev_labels, epochs=100, batch_size=8192)\n",
        "predictions, accuracy, r2 = evaluate_model(model, test_data, test_labels)"
      ],
      "metadata": {
        "id": "wIsFf2zVmTBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX4S5oZz_hxk"
      },
      "outputs": [],
      "source": [
        "i = 1000\n",
        "size = 10\n",
        "\n",
        "mean = test_labels[i:i + size, 2]\n",
        "std = test_labels[i:i + size, 3]\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions[i:i + size, 0] * std + mean)\n",
        "\n",
        "print(\"Corresponding actual values:\")\n",
        "print(test_labels[i:i + size, 1] * std + mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvM_xYomL-Q-"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "num_models=10\n",
        "\n",
        "def hyperparam_search(\n",
        "    num_models: int,\n",
        "    train_sequences,\n",
        "    train_labels,\n",
        "    val_sequences,\n",
        "    val_labels,\n",
        "    test_sequences,\n",
        "    test_labels,\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=mse_loss,\n",
        "    metrics=[dir_acc],\n",
        "):\n",
        "\n",
        "  input_shape = train_sequences.shape[1:]\n",
        "  head_size_arr = [64, 128, 256, 512] #256\n",
        "  num_heads_arr = [8, 16, 32, 64] #16\n",
        "  ff_dim_arr = [256, 512, 1024]#, 2048] # 1024\n",
        "  num_layers_arr = [8, 10, 12, 14, 16, 18] # 12\n",
        "  dropout_arr = [0, 0.05, 0.10, 0.20, 0.3, 0.4, 0.5, 0.6] # 0.20\n",
        "  batch_size_arr = [32, 64, 128]\n",
        "\n",
        "  best_accuracy = 0\n",
        "  best_params = None\n",
        "\n",
        "  for i in range(num_models):\n",
        "    try:\n",
        "      batch_size = random.choice(batch_size_arr)\n",
        "      head_size = random.choice(head_size_arr)\n",
        "      num_heads = random.choice(num_heads_arr)\n",
        "      ff_dim = random.choice(ff_dim_arr)\n",
        "      num_layers = random.choice(num_layers_arr)\n",
        "      dropout = random.choice(dropout_arr)\n",
        "\n",
        "      model = compile_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout, optimizer, loss, metrics)\n",
        "      train_model(model, train_sequences, train_labels, val_sequences, val_labels, epochs, batch_size)\n",
        "      predictions, accuracy, r2 = evaluate_model(model, test_sequences, test_labels)\n",
        "\n",
        "      if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = (input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
        "\n",
        "      write = f\"R-squared: {r2}, accuracy: {accuracy}, batch_size: {batch_size}, head_size: {head_size}, num_heads: {num_heads}, ff_dim: {ff_dim}, num_layers: {num_layers}, dropout: {dropout}\"\n",
        "      print(write)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Exception: {e}, continuing\")\n",
        "\n",
        "    best = f\"best params: {best_params}, best accuracy: {best_accuracy}\"\n",
        "    print(best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7-h0QEr6vsO"
      },
      "outputs": [],
      "source": [
        "hyperparam_search(\n",
        "    num_models,\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    dev_data,\n",
        "    dev_labels,\n",
        "    test_data,\n",
        "    test_labels,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=mse_loss,\n",
        "    metrics=[dir_acc],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}